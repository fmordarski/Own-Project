---
title: "Online Shopper's intention"
author: "Filip Mordarski"
date: "10/1/2019"
output: 
  pdf_document:
toc: true
toc_depth: 2
number_sections: true
fontsize: 11pt
geometry: margin=1in
---


```{r, include=FALSE}
options(tinytex.verbose = TRUE)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(rmarkdown)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("data.table", repos = "http://cran.us.r-project.org")

```

# Introduction

This report is part of the capstone project of the EdX course 'HarvardX: PH125.9x Data Science: Capstone'. Participants had to create their own project. Firstly, they need to seek out a dataset. I chose Online Shopper's Intention database from Kaggle. URL: [Link](https://www.kaggle.com/roshansharma/online-shoppers-intention.) 

## Executive Summary

The main goal of this project is building algorithms to predict if given online shopper will be generated revenue or not. It can be really useful information for entrepreneur. Online Shopper's Intention dataset contains around 12 thousands sessions of online shoppers. The information from the analysis of this database is used to generate revenue predictions that are compared with actual zero one variable revenue to check the quality of the forecasting algorithm. 

## Overview

This report is split in four sections. First, **Introduction** describes content of data set, summarizes the goal of the project and key steps that were performed. **Analysis** section explains the process and techniques used, such as data cleaning, data exploration and visualization and my modeling approach. In a **Results** section I present the modeling results and discuss the model performance. **Conclusion** section gives us a brief summary of the report, its limitations and future work.

## Data set description

The dataset consists of feature vectors belonging to 12,330 sessions. The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period. The dataset consists of 10 numerical and 8 categorical attributes.

* Administrative, Administrative Duration, Informational, Informational Duration, Product Related and Product Related Duration

These variables represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories.

* Bounce Rate

Feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.

* Exit Rate

Feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session.

* Page Value

Feature represents the average value for a web page that a user visited before completing an e-commerce transaction.

* Special Day

Feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.

* Month

Attribute represents which month was during the visit.

* Operating System, Browser, Region, Traffic type

Attributes represent what operating system (8 different operating systems) and browser (13 different browsers) online shoppers were using. Region (9 different regions) and traffic type (20 different types) indicate region of the user and traffic type during shopping. We can not say anything more about these attributes, because they are numeric values. We can only determine their relationships between them and the impact on the dependent variable.

* Visitor type

Feature indicates visitors as returning or new visitor.

* Weekend 

Boolean value indicating whether the date of the visit is weekend, and month of the year.

* Revenue

Attribute indicates if visitor will be generated revenue or not.


```{r ,echo = FALSE}
# provide below path to working directory
setwd("C:/Users/Uzytkownik/Documents/R courses edX/Harvard/capstone/own project")

#loading data
data <- read.csv("online_shoppers_intention.csv",header=TRUE)
```


# Analysis

This section expains the process and techniques used. It shows us some data visualization or data exploration. This section explains also modelling approach.

## Data cleaning

Firstly I would like to remove all rows with any NA values. The code below.

```{r }
# removing any rows with NA value
data <- data[complete.cases(data), ]
```


Now I am prepared to create train and testset for my algorithm.

```{r }
y <- data$Revenue

# Create trainset and test set 
set.seed(1)
test_index <- createDataPartition(y, times = 1, p = 0.1, list = FALSE)
test_set <- data[test_index, ]
train_set <- data[-test_index, ]
```



## Data exploration and visualization

Now we can begin the eye-pleasing part of the report. The below section will be full of plots and tables.


```{r ,echo = FALSE}
table_freq <- data %>%
  summarize(FreqPercTrue=mean(Revenue),FreqPercFalse=1-mean(Revenue)) 

kable(table_freq, align = 'c', caption = "What percentage of visitors generate revenue?", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) 
```

Above table shows us what percentage of visitors generate revenue or not.

Below we can see pie chart which shows us the above data in a pleasant view.


```{r ,echo = FALSE,message=FALSE}
# pie chart

bp <- ggplot(melt(table_freq), aes(x="",y=value,fill=variable)) +
  geom_bar(width = 1, stat = "identity") +
  ggtitle("Did the visitor generate revenue?")

pie <- bp + coord_polar("y", start=0)

# create blank theme
blank_theme <- theme_minimal()+
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.border = element_blank(),
  panel.grid=element_blank(),
  axis.ticks = element_blank(),
  plot.title=element_text(size=14, face="bold")
  )
#create nice chart

pie + scale_fill_brewer("Legend", label = c(FreqPercTrue = "Yes", FreqPercFalse = "No")) + 
  blank_theme +
  theme(axis.text.x=element_blank()) 

```

From the previous table and plot indicates that about 85 % sessions were not generated any revenue.

The below part and table show us counts of sessions that generated revenue or not. It represents that the month in which the most recorded sessions was May. However, in November the most sessions generated revenue.

```{r ,echo = FALSE,message=FALSE}
# Sort factor month in reverse order ("Feb","Mar","May","June","Jul","Aug","Sep","Oct","Nov","Dec") 

data$Month = factor(data$Month,levels(data$Month)[c(2,8,9,10,1,4,5,7,6,3)])

group_month <- data %>%
  group_by(Month) %>%
  summarise(True=sum(Revenue=='TRUE'),False=sum(Revenue=='FALSE'),Total=n()) 

kable(group_month, align = 'c', caption = "Revenue counts by month", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) 

data %>% ggplot(aes(Month))  + 
  geom_bar(aes(fill = Revenue), position = position_stack(reverse = TRUE)) +
  coord_flip() +
  scale_fill_brewer("Revenue") +
  theme(axis.text.x=element_blank()) +
  ggtitle("Revenue counts by month") +
  blank_theme 

```
\newpage


```{r, echo=FALSE, message=FALSE}
# Create plot and dataframe with counts of Revenue grouped by Visitor Type

data %>% ggplot(aes(VisitorType))  + 
  geom_bar(aes(fill = Revenue), position = position_stack(reverse = TRUE)) +
  coord_flip() +
  scale_fill_brewer("Revenue") +
  theme(axis.text.x=element_blank()) +
  ggtitle("Revenue counts by Visitortype") +
  blank_theme 

group_visitor <- data %>%
  group_by(VisitorType) %>%
  summarise(True=sum(Revenue=='TRUE'),False=sum(Revenue=='FALSE'),FreqPercTrue=round(sum(Revenue=='TRUE')/n(),2),Total=n()) 

kable(group_visitor, align = 'c', caption = "Revenue counts by month", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) 
```

The above chart and table may seem surprising. Follows from them new visitor generated revenue more often than returning visitor. Type 'new visitor' generated revenue in 25 % sessions, 'other' 19 % and 'returning visitor' only 14 %. It can be really useful information for online shop.

```{r, echo=FALSE, message=FALSE}
# Create dataframe with freq perc of revenue group by special day value

special_day <- data %>%
  group_by(SpecialDay) %>%
  summarise(CountsAllSessions=n(),FreqPercY=round(sum(Revenue=='TRUE')/n(),2))


special_day %>% ggplot(aes(SpecialDay,FreqPercY)) +
  geom_line(colour = "lightblue",size=2) +
  ggtitle("Frequency Percentage True Revenue vs Special Day indicator") +
  theme_minimal()

kable(special_day, align = 'c', caption = "Frequency Percentage True Revenue by Special Day", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

We can see from the above table and chart that the highest frequency percentage when session generate revenue is for Special Day equals 0. It also may seem surprising that the indicator is relatively low when the closeness of the site visiting time to a specific special day is relatively high.

``` {r, echo=FALSE, message=FALSE}
# Create dataframe with true revenue counts group by region

group_region <- data %>%
  group_by(Region) %>%
  summarise(CountsRevenueY=sum(Revenue=='TRUE'), FreqPercY = round(sum(Revenue=='TRUE')/n(),2))

data %>% ggplot(aes(Region))  + 
  geom_bar(aes(fill = Revenue)) +
  scale_fill_brewer("Revenue") +
  theme(axis.text.x=element_blank()) +
  ggtitle("Revenue counts by Region") +
  blank_theme 


kable(group_region, align = 'c', caption = "Revenue counts by Region", digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

The above table and plot represent statistics for Revenue grouped by region. we can infer from them that the most often region was Region 1. The most sessions that generated revenue were also in Region 1. 

## Modelling approach

I will use in my algorith classification method to predict if session generate revenue or not. The method 'Random Forest' from 'caret' package will be used. Below section was wrote based on 'Introduction to Data Science' written by Rafael A. Irizarry.

Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.

The first step is bootstrap aggregation or bagging. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the individual trees are not the same, we use the bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. 

So firstly what we need to do is convert type of columns 'Revenue' in trainset and testset to factors.

```{r}
train_set$Revenue <- as.factor(train_set$Revenue)
test_set$Revenue <- as.factor(test_set$Revenue)
```

Now we can train our trainset using Random Forest method.

```{r}
train_rf <- train(Revenue~., method = 'rf',data = train_set)
```

# Results

In this section I will present the modeling results and discuss the model performance.

## Cross validation

We can quickly see the results of the cross validation using the ggplot function. 

``` {r, echo=FALSE}
ggplot(train_rf) +
  geom_line(colour = "lightblue",size=2) +
  ggtitle("Cross validation") +
  theme_minimal()


```

From the above plot we can see that the smallest mtry value is in the range of 10 and 15. If we execute below code we can see the specific value of mtry (Randomly selected predictors).

``` {r}
train_rf$finalModel$mtry

```

## Algorithm efficiency

Now we are ready to predict our data in testset. Below code allows us to do this.

``` {r}
y_hat_knn <- predict(train_rf, test_set,type = "raw")

```

Below code informs us about our algorithm efficiency.

``` {r}
confusionMatrix(data=y_hat_knn, reference=test_set$Revenue)
```

If we would like to extract only Accuracy of our algorithm we can do this.

```{r}
confusionMatrix(data=y_hat_knn, reference=test_set$Revenue)$overall["Accuracy"]
```

# Conclusion

The aim of the project was to predict if given online shopper will be generated revenue or not. My algorithm achieved nice result of accuracy of **0.914**. This algorithm can be very useful for an entrepreneur running an online store to predict profits.  